{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankit071105/ATS-Score/blob/main/AtsScore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K8DEuriTp20U"
      },
      "outputs": [],
      "source": [
        "!pip install -q scikit-learn pandas nltk python-docx pdfminer.six plotly gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L-QMWtbSp20V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from pdfminer.high_level import extract_text\n",
        "from docx import Document\n",
        "import io\n",
        "from collections import defaultdict\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import gradio as gr\n",
        "import tempfile\n",
        "import os\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2U-DWmTp20V",
        "outputId": "b0c9a3c5-f09b-48d7-e0ec-20468f0e0d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3HZFqncwp20V"
      },
      "outputs": [],
      "source": [
        "# 1.  ATS Scoring Model\n",
        "class ATSModel:\n",
        "    def __init__(self):\n",
        "        # Initialize with sample data\n",
        "        self.sample_data = pd.DataFrame({\n",
        "            'text': [\n",
        "                \"Experienced Python developer with 5+ years building web applications. Skills: Python, Django, SQL. Education: BS Computer Science.\",\n",
        "                \"Marketing manager with 7 years experience. Led teams of 10+ people. Skills: SEO, PPC, Social Media. Education: MBA Marketing.\",\n",
        "                \"Recent computer science graduate. Projects: Machine learning models. Skills: Python, TensorFlow. Education: BS Computer Science.\",\n",
        "                \"Graphic designer with portfolio. Skills: Photoshop, Illustrator. Education: BFA Design.\"\n",
        "            ],\n",
        "            'score': [88, 85, 76, 82]\n",
        "        })\n",
        "        self.vectorizer = TfidfVectorizer(max_features=200)\n",
        "        self.model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "        self._train_model()\n",
        "\n",
        "    def _train_model(self):\n",
        "        # Train on sample data\n",
        "        X = self.vectorizer.fit_transform(self.sample_data['text'])\n",
        "        y = self.sample_data['score']\n",
        "        self.model.fit(X, y)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        text = text.lower()\n",
        "        text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
        "        words = text.split()\n",
        "        words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def extract_features(self, text):\n",
        "        processed = self.preprocess_text(text)\n",
        "\n",
        "        # Check for important sections\n",
        "        sections = ['experience', 'education', 'skills', 'projects', 'summary']\n",
        "        section_presence = {f\"has_{sec}\": int(sec in processed) for sec in sections}\n",
        "        keywords = {\n",
        "            'technical': ['python', 'java', 'sql', 'machine learning', 'algorithms'],\n",
        "            'soft': ['communication', 'teamwork', 'leadership', 'problem solving'],\n",
        "            'education': ['degree', 'university', 'gpa', 'coursework']\n",
        "        }\n",
        "        keyword_counts = {k: sum(1 for kw in v if kw in processed) for k,v in keywords.items()}\n",
        "        word_count = len(processed.split())\n",
        "\n",
        "        return {\n",
        "            **section_presence,\n",
        "            **keyword_counts,\n",
        "            'word_count': word_count\n",
        "        }\n",
        "\n",
        "    def predict_score(self, text):\n",
        "        processed = self.preprocess_text(text)\n",
        "        vec = self.vectorizer.transform([processed])\n",
        "        return float(np.clip(self.model.predict(vec)[0], 0, 100))\n",
        "ats_model = ATSModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qx8PKyu8p20V"
      },
      "outputs": [],
      "source": [
        "# 2. Feedback Generator\n",
        "def generate_feedback(score, features):\n",
        "    feedback = []\n",
        "    if score >= 85:\n",
        "        feedback.append(\"‚úÖ Excellent! Your resume is well-optimized for ATS systems.\")\n",
        "    elif score >= 70:\n",
        "        feedback.append(\"üü° Good. Your resume could use some improvements to score higher.\")\n",
        "    else:\n",
        "        feedback.append(\"üî¥ Needs work. Significant improvements needed for better ATS performance.\")\n",
        "    missing_sections = [s.replace('has_', '') for s,v in features.items()\n",
        "                       if s.startswith('has_') and not v]\n",
        "    if missing_sections:\n",
        "        feedback.append(f\"‚ö†Ô∏è Missing sections: {', '.join(missing_sections)}\")\n",
        "    if features['technical'] < 3:\n",
        "        feedback.append(\"‚ö†Ô∏è Add more technical skills relevant to the job\")\n",
        "    if features['soft'] < 2:\n",
        "        feedback.append(\"‚ö†Ô∏è Include more soft skills like communication and teamwork\")\n",
        "    if features['word_count'] < 300:\n",
        "        feedback.append(\"‚ö†Ô∏è Resume is too short - add more details about your experience\")\n",
        "    elif features['word_count'] > 800:\n",
        "        feedback.append(\"‚ö†Ô∏è Resume is too long - focus on most relevant information\")\n",
        "    feedback.append(\"üí° Tip: Use bullet points to describe your experience\")\n",
        "    feedback.append(\"üí° Tip: Quantify achievements with numbers when possible\")\n",
        "\n",
        "    return feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QcMu8YOzp20W"
      },
      "outputs": [],
      "source": [
        "# 3. Visualization Components\n",
        "def create_visualizations(score, features):\n",
        "    fig_gauge = go.Figure(go.Indicator(\n",
        "        mode=\"gauge+number\",\n",
        "        value=score,\n",
        "        domain={'x': [0, 1], 'y': [0, 1]},\n",
        "        title={'text': \"ATS Score\"},\n",
        "        gauge={\n",
        "            'axis': {'range': [0, 100]},\n",
        "            'steps': [\n",
        "                {'range': [0, 60], 'color': \"red\"},\n",
        "                {'range': [60, 80], 'color': \"yellow\"},\n",
        "                {'range': [80, 100], 'color': \"green\"}],\n",
        "            'threshold': {\n",
        "                'line': {'color': \"black\", 'width': 4},\n",
        "                'thickness': 0.75,\n",
        "                'value': score}\n",
        "        }))\n",
        "    keyword_data = {'Technical': features['technical'],\n",
        "                   'Soft Skills': features['soft'],\n",
        "                   'Education': features['education']}\n",
        "    fig_keywords = px.bar(\n",
        "        x=list(keyword_data.keys()),\n",
        "        y=list(keyword_data.values()),\n",
        "        title=\"Keyword Coverage\",\n",
        "        labels={'x': 'Category', 'y': 'Count'}\n",
        "    )\n",
        "    section_data = {'Experience': features['has_experience'],\n",
        "                   'Education': features['has_education'],\n",
        "                   'Skills': features['has_skills'],\n",
        "                   'Projects': features['has_projects'],\n",
        "                   'Summary': features['has_summary']}\n",
        "    fig_sections = px.bar(\n",
        "        x=list(section_data.keys()),\n",
        "        y=list(section_data.values()),\n",
        "        title=\"Section Presence\",\n",
        "        labels={'x': 'Section', 'y': 'Present (1) or Missing (0)'},\n",
        "        range_y=[0, 1]\n",
        "    )\n",
        "\n",
        "    return fig_gauge, fig_keywords, fig_sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kC8tcj20p20W"
      },
      "outputs": [],
      "source": [
        "# 4. Universal File Handler for All Gradio Versions\n",
        "def handle_uploaded_file(file_obj):\n",
        "    # Create temporary directory\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    file_path = os.path.join(temp_dir, \"uploaded_resume\")\n",
        "\n",
        "    try:\n",
        "        if isinstance(file_obj, str):\n",
        "            if file_obj.endswith('.pdf'):\n",
        "                resume_text = extract_text(file_obj)\n",
        "            elif file_obj.endswith('.docx'):\n",
        "                doc = Document(file_obj)\n",
        "                resume_text = '\\n'.join([para.text for para in doc.paragraphs])\n",
        "            else:\n",
        "                with open(file_obj, 'r', encoding='utf-8') as f:\n",
        "                    resume_text = f.read()\n",
        "            return resume_text\n",
        "\n",
        "        elif hasattr(file_obj, 'read'):\n",
        "            file_bytes = file_obj.read()\n",
        "            if file_obj.name.endswith('.pdf'):\n",
        "                resume_text = extract_text(io.BytesIO(file_bytes))\n",
        "            elif file_obj.name.endswith('.docx'):\n",
        "                doc = Document(io.BytesIO(file_bytes))\n",
        "                resume_text = '\\n'.join([para.text for para in doc.paragraphs])\n",
        "            else:\n",
        "                resume_text = file_bytes.decode('utf-8')\n",
        "            return resume_text\n",
        "\n",
        "        elif isinstance(file_obj, dict):\n",
        "            file_bytes = base64.b64decode(file_obj['data'].split(',')[1])\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(file_bytes)\n",
        "\n",
        "            if file_obj['name'].endswith('.pdf'):\n",
        "                resume_text = extract_text(file_path)\n",
        "            elif file_obj['name'].endswith('.docx'):\n",
        "                doc = Document(file_path)\n",
        "                resume_text = '\\n'.join([para.text for para in doc.paragraphs])\n",
        "            else:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    resume_text = f.read()\n",
        "            return resume_text\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file object type\")\n",
        "\n",
        "    finally:\n",
        "        if os.path.exists(file_path):\n",
        "            os.remove(file_path)\n",
        "        if os.path.exists(temp_dir):\n",
        "            os.rmdir(temp_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kTVsQ3cXp20W"
      },
      "outputs": [],
      "source": [
        "# 5. Main Analysis Function\n",
        "def analyze_resume(file_obj):\n",
        "    try:\n",
        "        resume_text = handle_uploaded_file(file_obj)\n",
        "        score = ats_model.predict_score(resume_text)\n",
        "        features = ats_model.extract_features(resume_text)\n",
        "        feedback = generate_feedback(score, features)\n",
        "        fig_gauge, fig_keywords, fig_sections = create_visualizations(score, features)\n",
        "        feedback_html = \"<h3>Recommendations</h3><ul>\" + \\\n",
        "                       \"\".join([f\"<li>{item}</li>\" for item in feedback]) + \"</ul>\"\n",
        "\n",
        "        metrics_html = f\"\"\"\n",
        "        <h3>Resume Metrics</h3>\n",
        "        <p><b>Word Count:</b> {features['word_count']} (300-800 recommended)</p>\n",
        "        <p><b>Technical Keywords:</b> {features['technical']} found</p>\n",
        "        <p><b>Soft Skills:</b> {features['soft']} found</p>\n",
        "        <p><b>Missing Sections:</b> {sum(1 for k,v in features.items() if k.startswith('has_') and not v)}</p>\n",
        "        \"\"\"\n",
        "\n",
        "        return fig_gauge, fig_keywords, fig_sections, feedback_html, metrics_html\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"<div style='color:red;padding:20px;border:1px solid red;border-radius:5px;'>Error processing file: {str(e)}</div>\"\n",
        "        empty_fig = go.Figure()\n",
        "        empty_fig.update_layout(title=\"Error occurred\", showlegend=False)\n",
        "        return empty_fig, empty_fig, empty_fig, error_msg, \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g1FR8gZmhzt2"
      },
      "outputs": [],
      "source": [
        "# 6. Gradio Interface with Robust Error Handling\n",
        "with gr.Blocks(title=\"ATS Resume Analyzer\", theme=gr.themes.Soft()) as interface:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üöÄ AI-Powered ATS Resume Analyzer\n",
        "    Upload your resume to get an instant ATS score and personalized improvement recommendations\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            file_input = gr.File(\n",
        "                label=\"Upload Resume (PDF or DOCX)\",\n",
        "                file_types=[\".pdf\", \".docx\", \".txt\"],\n",
        "                type=\"filepath\"\n",
        "            )\n",
        "            submit_btn = gr.Button(\"Analyze Resume\", variant=\"primary\")\n",
        "\n",
        "            # Deployment options\n",
        "            with gr.Accordion(\"Deployment Options\", open=False):\n",
        "                gr.Markdown(\"\"\"\n",
        "                ### For permanent hosting:\n",
        "                [![Hugging Face Spaces](https://img.shields.io/badge/Deploy%20to-Hugging%20Face%20Spaces-blue)](https://huggingface.co/spaces)\n",
        "                \"\"\")\n",
        "        with gr.Column():\n",
        "            gauge_plot = gr.Plot(label=\"ATS Score\")\n",
        "\n",
        "    with gr.Row():\n",
        "        keywords_plot = gr.Plot()\n",
        "        sections_plot = gr.Plot()\n",
        "\n",
        "    with gr.Row():\n",
        "        feedback_output = gr.HTML()\n",
        "        metrics_output = gr.HTML()\n",
        "\n",
        "    submit_btn.click(\n",
        "        analyze_resume,\n",
        "        inputs=file_input,\n",
        "        outputs=[gauge_plot, keywords_plot, sections_plot, feedback_output, metrics_output]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "CNEBdv7mp20W",
        "outputId": "79027b59-9a18-4ff4-e3de-7cccf01897a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d3b0bf9816c9225e2d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d3b0bf9816c9225e2d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Launch the app\n",
        "try:\n",
        "    interface.launch(share=True, debug=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error launching Gradio interface: {str(e)}\")\n",
        "    print(\"Trying alternative launch method...\")\n",
        "    interface.launch(share=False, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}